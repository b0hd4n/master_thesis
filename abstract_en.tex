%%% A template for a simple PDF/A file like a stand-alone abstract of the thesis.
\pdfminorversion=5

\documentclass[12pt]{report}

\usepackage[a4paper, hmargin=1in, vmargin=1in]{geometry}
\usepackage[a-2u]{pdfx}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{textcomp}

\begin{document}

%% Do not forget to edit abstract.xmpdata.

In international and highly-multilingual environments,
it often happens, that a talk, a document, or any other
input, needs to be translated into a huge number of other languages.
Hovewer, it is not always an option to have a distinct system for each
possible sentence pair due to the fact that such kind of translation
systems are computationaly demanding.

Combining multiple target languages into one translation model usually
causes a decrease in quality of output for each its translation
direction.
In this thesis we experiment with combinations of target languages
to see, if specific grouping of them can lead to better results,
than just randomly selecting target languages.

We make use of recent researches about training a multilingual
Transformer model without any change to its architecture:
adding a target language tag to the source sentence.

We trained a number of bilingual and multilingual
Transformer models and evaluated them on multiple test sets
from different domains.
We found, that in most of the cases, grouping related
target languages into one model caused better performance
compared to models with randomly selected languages.
However, when comparing any of tried multilingual models
with bilingual ones, we noticed that the domain of the test
set as well as domains of datasets used for sampling
the training data for each language pair, might have a higher
effect that the grouping of target languages.

\end{document}

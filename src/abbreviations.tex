% https://www.overleaf.com/learn/latex/glossaries
% example:
% \acrshort{mt} - MT
% \acrlong{mt} - machine translation
% \acrfull{mt} - MT (machine translation)

% \Gls{latex} - Latex
% \gls{latex} - latex
% \GLS{latex} - LATEX
% \glspl{term} - terms

\makeglossaries

\newglossaryentry{latex}
{
    name=latex,
    description={Is a mark up language specially suited 
    for scientific documents}
}

\newglossaryentry{loss}
{
    name=loss,
    description={Loss function, also often called 'objective function' and
		 'error function'. It is optimized during the training
		 process. In our experiments it is mean word cross-entropy score.}
}

\newglossaryentry{baseline}
{
    name=baseline,
    description={In machine learning this term refers to a simple or
		 na√Øve initial solution, which efficiency it then
		 taken as a reference point and later improved.
		}
}

\newglossaryentry{gradient-descent}
{
    name=gradient descent,
    description={An algorithm of iterative optimization of 
		 differentiable objective function (loss)}
}

\newglossaryentry{stochastic-gradient-descent}
{
    name=stochastic gradient descent,
    description={Method for iterative optimizing an objective function.
		 Differs from gradient descent by approximating the gradient
		 on mini-batch of data.}
}

\newglossaryentry{epoch}
{
    name=epoch,
    description={Refers to one pass of full training dataset to the learning 
		 algorightm
		}
}

\newglossaryentry{early-stopping}
{
    name=early stopping,
    description={Regularization technique to avoid model overfit.
		 Usualy consists of stopping the training process
		 when the value of some selected metric on the 
		 validation set is not improved for last number of
		 validation steps.
		}
}

\newglossaryentry{overfitting}
{
    name=overfitting,
    description={
		 Occurs when the model's performance on unseen validation
		 set stops improving while on the training set
		 it still improves.
		}
}

\newglossaryentry{en-to-5}
{
    name=en-to-5,
    description={The dataset created from UN parallel corpus,
	with source sentences in English and target sentences
	in one of following 5 languages: Spanish, French, Russian,
	Arabic and Chinese; described in Section \ref{subsection:en-to-5}
	}
}

\newglossaryentry{en-to-36}
{
    name=en-to-36,
    description={The dataset with source sentences in English
	and target sentences in 36 languages, described
	in Section \ref{subsection:en-to-36}
	}
}

\newacronym{mt}{MT}{machine translation}
\newacronym{smt}{SMT}{statistical machine translation}
\newacronym{nmt}{NMT}{neural machine translation}
\newacronym{sgd}{SGD}{stochastic gradient descent}
%%% \newacronym{}{}{}
%%% WMT18, WMT19, WMTxx -- annual Workshop on Statistical Machine Translation
%%% (year 2018, 2019, 20xx resp.)
\newacronym{rnn}{RNN}{recurrent neural network}
\newacronym{cnn}{CNN}{convolutional neural network}
\newacronym{lstm}{LSTM}{long short-term memory}
% --  (RNN architecture)
\newacronym{gru}{GRU}{gated recurrent unit}
\newacronym{bpe}{BPE}{byte pair encoding}
\newacronym{alpac}{ALPAC}{Automatic Language Processing Advisory Committee}
\newacronym{arpa}{ARPA}{Advanced Research Projects Agency}
\newacronym{bleu}{BLEU}{bilingual evaluation understudy}
%--  (method of automatic MT evaluation)
%%% \newacronym{}{}{}

\chapter{Experiment setup}
\label{chapter:experiment_setup}

In this chapter, we describe the data used for experiments, training setup
and experiments that were run to answer the questions asked in this thesis.

%----------------------------------------------------------------------
\section{\todo{Questions and constraints}}
\label{section:questions_and_constraints}

Constraints:
\begin{displayquote}
	Translation quality for multi-lingual system is better or insignificantly
	worse than for mono-lingual one-to-one tranlsation system.

	Maximum possible target languages are combined in one model.
\end{displayquote}

Questions:
\begin{displayquote}
	How, \emph{on average}, does adding one more randomly selected target language
	to the multitarget model affect its En\to{}De performance?

	How is it different if we add a linguistically similar,
	not a randomly selected language?

	How does adding one more language from the same language family or group 
	\emph{on average} affect translation performance for a selected language
	pair (e.g. En\to{}De)?
\end{displayquote}



%----------------------------------------------------------------------
\section{Experiments}
\label{section:experiments}
\subsection{Starting point}
\label{subsection:starting_point}

The approach described in \cref{section:multitarget_theory} with combining multiple
translation directions into the standart \emph{Transformer} model can be also used to
train just multi-target models, i.e. with one source language and multiple target languages.
The following papers
(\perscite{arivazhagan-2019-mmnmt-in-the-wild}, \perscite{aharoni-etal-2019-massively}),
which further develop the approach, describe and try many different interesting cases.
However, in each setting there is usually only one model of each kind considered.
For example, when in \citet{aharoni-etal-2019-massively} compares 5-to-5,
25-to-25, 50-to-50, etc. models, there is only one 5-to-5 model, one 25-to-25, etc.

To conduct our experiments, we use this approach, but with the following differences:
\begin{itemize}
	\item We fix English as a source language, as we are exploring the multi-target experiments only, .
	\item For every translation direction and every setting we train multiple models.
	E.g. for the \dir{En}{De} translation direction and 1-to-5 setting there are
	couple of \dirmany{En}{De + 4 randomly selected targets} models.
	\item We use only up to 5 target languages in the model because of:
	\begin{itemize}
		\item limited resources;
		\item our selected datasets (which will be described in the next section)
		do not contain more than 5-6 languages of the same language group.
	\end{itemize}
\end{itemize}



\subsection{Proposed experiments}
\label{subsection:proposed_experiments}

% Given the questions and constraints given in \ref{section:questions_and_constraints},
% the variable object in experiments is the data itself. Due to that, the setup similar
% to \cite{johnson-etal-2017-googles} was chosen. 

\subsubsection*{Bilingual \gls{baseline}}

Bilingual models. The purpose is to have a reference point to be able to reason how
does every additional target language affects the model's performance.
\perscite{Siddhant2019} shows that using target language tags results
in the same model efficiency as separately encoding the target.
Therefore, we use target tags in this setting too, so that we can use
the same training pipeline.

\subsubsection*{Multi-lingual baselines (RANDOM)}

Multilingual models with a random set of target languages.
The purpouse is twofold: 
to show \acrshort{bleu} score decrease with increasing number of target languages and
to serve as a baseline for multitarget models with target languages grouped by
in non-random way, e.g. by language group or linguistic similarity.


\subsubsection*{Group by language group (SIMILAR)}

Multilingual models with a set of target languages from the same language group.
Due to shared parts of vocabulary and linguistic properties we expect to
see better results than for multi-lingual baselines.
Ideally the results could be comparable with bilingual baselines.


% \subsubsection*{Group by linguistic similarity}
% 
% From \perscite{siddhant-2020-x-ling-effect} follows that languages' script
% and similarly the amount of shared vocabulary is not so important
% for XX\to{}En translation direction.
% Example with Serbian and Croatian, with the same vocabulary but
% in different scripts.


%----------------------------------------------------------------------
\section{Dataset(s)}
\label{section:datasets}


\subsection{\edit{English to 36 languages}}
\label{subsection:en-to-36}

To observe effects of linguistic similarity of target languages,
it is important to examine enough possible variations of those.
The OPUS dataset (\cite{TIEDEMANN12.463}) is an open and free collection of texts
that covers more than 90 languages with data from several
domains.\footnote{Available at \url{http://opus.nlpl.eu/}} 

For our experiments the source language is English only.

Given the list of target languages in this dataset (see full list in \cref{att:list_en-to-36}),
we decided to select these two groups of languages for the SIMILAR experiment:
\begin{itemize}
	\item Germanic group: da, de, is, no, nl, sv.
	\item Slavic with cyrillic script: bg, mk, ru, uk.
\end{itemize}

We made use of the sampling and splitting of the data created by the ELITR project.%
\footnote{\url{https://elitr.eu/wp-content/uploads/2019/07/D11.FINAL\_.pdf}}
For each of the language pairs and each sub-dataset
the data was split to training, validation and testing sets.
For each of the two latter sets, 2000 random sentences were selected
and the rest of the data remained for the training set.
In cases where the sub-dataset contained less than 16000 sentence pairs,
no data went to the validation set.
Later, for each language pair there were 1000000 sentence pairs
sampled from all training sub-sets.
\fix{To be more explicit that the sampling is directed towards certain domains.
This is somewhat unclear.}
Firstly, if available, the sentences were taken from Europarl,
then EUbooks, OpenSubtitles, and then all remaining sub-datasets.
The same procedure was used to sample \todo{check} x000 of validation set sentences
per each language pair.
The test sets were left separate, so that the result on each domain would be observable.

Later we found an overlap in the source side of different language pairs.
Although this would not directly lead to unfair increase of the test score,
such sentence pairs were removed from the training sets.
This filtering decreased the number of sentence pairs
to 0.85-0.95 millions per language pair.
\todo{describe the figure with stats}
\todo{describe the table with groups of sub-datasets}
\todo{group 4: open folder, save file}
\todo{group 5: Tanzil - completely different domain, Books of 18th cent.-  dated vocabulary
	      Wikipedia - automaticaly aligned sentences.}


\begin{table}[h!]
	\centering
	\begin{tabular}{c|p{0.5\columnwidth}|p{0.4\columnwidth}}
	\toprule
	     group & subdataset names  & description \\
	\midrule
	 1 &  Europarl/vx, DGT, MultiUN, EUbookshop, JRC-Acquis,
	      ECB, EMEA
	   &  Proceedings and documents from Europarl, UN, etc. \\
	 2 &  NewsCommentary, GlobalVoices, WMT-News 
	   &  News articles and commentaries \\
	 3 &  OpenSubtitles, Tatoeba
	   &  Short sentences, human speech, general domain \\
	 4 &  OpenOffice, PHP, KDE4, Gnome
	   &  Software documentation or interface elements \\
	 5 &  Tanzil, Books, Wikipedia
	   &  Other  \\
	\bottomrule
	\end{tabular}

	\mycaption{Groups of subdatasets in OPUS}{}
	\label{tab:subdatasets_groups}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/train_set_statistics.png}
	\mycaption{Training data language statistics}{
		Languages are on the $X$ axis sorted as in \cref{att:list_en-to-36}.
		From top to bottom:
		total number of sentence pairs in training set per language,
		average number of subwords per sentence on the source side,
		the same on the target side,
		total number of unique subwords for this target language on the source side,
		the same on the target side.
	}
	\label{fig:language_statistics}
\end{figure}



\subsection{\todo{UN parallel corpus: English to 5 languages}}
\label{subsection:en-to-5}

\todo{as I show en-to-5 results in RANDOM section, this DS should be here}
\cite{eisele-chen-2010-multiun}


%----------------------------------------------------------------------
\section{Method}
\label{section:method}

In this section we describe how the models are trained, which metrics
are collected and how are they analyzed.


\subsection{\todo{Data preprocessing}}
\label{section:data_preprocessing}

\todo{1. BPE is used, same vocabulary for all models from en-to-36 setup}


\subsection{\todo{Data selection}}
\label{section:data_selection}

\todo{move here resp. parts from training, validation and test}


\subsection{\todo{Training tasks}}
\label{section:training_tasks}

\todo{what is a task, multilingual task, sampling tasks for RANDOM and SIMILAR}


\subsection{\todo{Training}}

For example, let us take \dirmany{En}{Fr, De} setup, which means that
the model to be trained should take a source sentence in English and
produce translation either in French or in German.
The language of model's output depends on the target tag at the beginning
of the input sentence, i.e. \tagto{fr} tag in source sentence leads to French
target.

To train such a model, only related sentence pairs are subsampled
from the whole training set.
In this case, from the whole training set we select only those sentence
pairs which source side starts with tags \tagto{fr} or \tagto{de}.
Such subsampled dataset is then used to train the model.

During the training procedure, once per specified number of updates
occurs the checkpointing of the model.
The model weights are saved to the disk and number of measurements are logged.

\begin{samepage}
\begin{itemize}
	\item [Those measurements are:]
	\item training loss value (mean value for all updates since
	last checkpoint)
	\item learning rate value
	\item training speed (processed words per second)
	\item training time since last checkpoint
	\item number of updates happened from the beginning till this checkpoint
\end{itemize}
\end{samepage}
\begin{samepage}
Hardware usage should also be recorded if possible:
\begin{itemize}
	\item GPU usage
	\item CPU usage
	\item memory usage
	\item disk I/O
	\item network I/O
\end{itemize}
\end{samepage}

The hardware metrics are not important for model's evaluation
but may help early spot mistakes like underuse of GPU or CPU, lack of RAM, etc.
This is why they could possibly be recorded continuously. \todo{link to this point from wandb section}

\subsection{\edit{Validation}}
\label{subsection:validation}

The validation set is used to track model's performance during the training
on an unseen set of data and to perform early stopping.
These measurements are only used during the training and not for the evaluation.

Once per specified number of steps the validation occurs:
validation metrics are recorded, for any metric which value was
improved current model weights are saved as best model by this metric.
If early stopping condition occurred then the training process is stopped.

For any model the validation set is constructed from the big validation set
by selecting only relevant sentence pairs in the same way as the training set,
i.e. pairs with the target in one of the examined languages.
For the example setup from above, \dirmany{En}{De,Fr}, the validation set
consists of an equal amount of \dir{En}{De} and \dir{En}{Fr} sentence pairs.
E.g. if in the complete validation set there are 1000 sentence pairs for
each of possible target languages, then for \dirmany{En}{De,Fr}
model the validation set will contain 2000 sentence pairs, and for
\dirmany{En}{De,Es,Fr} it will contain 3000 sentence pairs.

For the validation set, we collect not only the loss function value
but also the metric of interest, which is \acrshort{bleu} score.
However, this \acrshort{bleu} scores are not used for the model's
evaluation but only during the training process.
The \acrshort{bleu} of the whole model's validation set
is not something we are interested in.
For the discussed example we collect validation bleu:fr and bleu:de scores
which represent \acrshort{bleu} scores for French and German
parts of validation set.
E.g., to compute bleu:fr we select only En\to{}Fr sentence pairs from the
validation set.

Also, an aggregated value of the bleu:xx scores, i.e. the mean of BLEU scores
over all target languages of the current model, is also recorded
and may be used for early stopping: ending the training process
when the metric is not improved during last N validation steps.

\begin{samepage}
Altogether, the following validation metrics are recorded after the
validation step:
\begin{itemize}
	\item loss function value
	\item bleu:xx which is \acrshort{bleu} score for each of
	model's target languages
	\item aggregated value of all bleu:xx values
	\item translation time of the model's validation set
\end{itemize}
\end{samepage}

\subsection{\todo{Finishing the training}}
\label{section:finishing-the-training}

When should we stop the training?
It is not possible to say precisely when did the model
acquire its best performance because of stochastic nature of
the training algorightm (\acrshort{sgd}).
Because of that we need to use some method to decide when
training process should be stopped.


\subsubsection*{Number of \glspl{epoch}}

\todo{First occurence of a term to be italic}
The easiest approach is to specify the number of \glspl{epoch}
after which the training is stopped.
This could be a good solution for the case when all models
that will be compared are trained on the same amount of data from
the same domain.
But in our case, adding one more target language adds a constant
amount of sentence pairs to the training set.
Roughly, if the number of \glspl{epoch} is specified as a stop
condition, a bilingual \dir{En}{De} model will see the German
training data $x$ times, when multilingual \dir{En}{De, Fr, Es}
will only see the German training data $x / 3$ times.

%What value should be used to compare model's performace in time?
%The first and the most obvious approach 
\subsubsection*{Early stopping}

\Gls{early-stopping} is a regularization technique used to avoid
possible \gls{overfitting} of a model on the training data.
In general, it works in the following way: after every validation step
it checks if the metric value improved during last $N$ validations.
The metric to be controlled and number of validation steps $N$ are
the parameters of this method
(see \cref{fig:early_stopping}).

Another situation is even more probable in the area of NMT with generally
large training datasets: model's validation performance
is either stalled or slightly improved
(see \cref{fig:early_stopping_no_improvement}).
In this case \gls{early-stopping} helps to avoid unnecessary spendings
on computational resources.

\begin{samepage}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/early_stopping.png}
	\mycaption{\Gls{early-stopping} to prevent overfitting}{
		At the `early stopping' point the model's performance
		on unseen validation set of data does not improve
		anymore. Further training leads to poorer performance
		on unseen data. Stopping the training at this point
		results in better model's performance on unseen data.
	}
	\label{fig:early_stopping}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/early_stopping_no_improvement.png}
	\mycaption{\Gls{early-stopping} as the model is not improving}{
		Even though the metric value on the training set is
		still slowly improving, the its value on the unseen
		validation set is stalled. Further spending of 
		computational resources is unjustified.
	}
	\label{fig:early_stopping_no_improvement}
\end{figure}
\end{samepage}

In our case we could use \gls{early-stopping} to ensure more equal conditions
for models with different sizes of training data.
A suitable number $N$ could be found experimentaly, but which metric should be used?

\todo{cross-entropy, perplexity; they may not represent the model's
performance, what about BLEU}

Given that the task is to train a model that is as good as possible in
\textbf{every} of its target directions, the \acrshort{bleu} score of
the whole translated validation set for this set of languages does not
say anything about the model's performance in each specified
translation direction.

\subsubsection*{Aggregated value of BLEU scores}

Therefore, we should use separate BLEU scores which represent
model's performance in each of translation directions.
The most intuitive and naive way is to compare BLEU scores
for each target language.

However, most of frameworks and toolkits can monitor only one metric
for the early stopping.
Considering that different validation \acrshort{bleu} scores
are computed for different parts of the validation set and in
which are in different languages, they cannot be directly compared
and may have different scale.

For example, a model for the \dirmany{En}{De,Fr} direction is being
trained.
Before the moment, an \dir{En}{De} model has already been trained and
had the best \acrshort{bleu} score of 25 on the German part of the
validation set.
A \dir{En}{Fr} model has also been trained, and its result on the
French part of the validation score is 35.
So for the currently training \dirmany{En}{De,Fr}, one percentage point
change for the \dir{En}{De} direction is not equal to the same change for the
\dir{En}{Fr} direction.

Geometric mean is known to be good for aggregating multiple metrics
with different scale (see \cref{eq:geometric_mean}).

\XXX{initially I wanted to achieve what was in the removed paragraph,
but it did not happen (in some cases one of BLEU scores goes a bit down),
so I have replaced it with this version}

\begin{equation}
\label{eq:geometric_mean}
	geometric\_mean = \left(\prod _{i=1}^{n}x_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}
\end{equation}

\subsection{\edit{Testing}}
\label{subsection:testing}

After the training is finished, the received models should be evaluated on unseen
test data.
For experiments with \gls{en-to-5} dataset (Section \ref{subsection:en-to-5})
the test sets are created in the same way as validation sets.
For \gls{en-to-36} dataset (Section \ref{subsection:en-to-36})
the test set is divided on subsets by the source dataset.
It means, that each of source datasets (like OpenSubtitles/v11, 
Europarl/v7, etc.) there exists a separate test set.
So after translating the test set for each of target directions of the model
the following record is created:
\begin{itemize}
	\item model name
	\item source language
	\item target languages
	\item tested target language
	\item \acrshort{bleu} score for this part of translation
	\item metric, based on which the best model was saved
	\item dataset name (for \gls{en-to-36})
\end{itemize}

Let us return to the example setup is En\to{}$\{$De, Fr$\}$ and
suppose the reported validation metrics are the mean loss function value on
test set and 'translation' (geometric mean of all reported BLEU scores,
see Section \ref{subsection:validation}).
After the training is finished, there will be two models: best by
loss value and best by 'translation'.
For each of those two records are created: for En\to{}Fr translation
and for En\to{}Es. In total, 4 results are recorded.

If the model was trained and tested on \gls{en-to-36} dataset,
than 4 times \textit{n} records are created, where \textit{n}
is number of OPUS subdatasets from which the data was sampled.


\subsection{Analysis}

After required set of models is trained and their test
metrics are collected, data should be analysed.

For example, let us take these four models: \dirmany{En}{De, Fr},
\dirmany{En}{De, Az}, \dirmany{En}{De, Bg}, and \dirmany{En}{Bg, Az}.
After the training, they provide us with three results for \dir{En}{De}
direction 2-target baseline,
one value for \dir{En}{Fr},
two values for \dir{En}{Bg}
and two for \dir{En}{Az}.
These aggregated \dirmany{En}{De, X} results will be later compared with
aggregated \dirmany{En}{De, X1, X2} for three target languages,
\dirmany{En}{De, X1, X2, X3} for four target languages, where X1, ... X$i$ are
some other targets.

Next, the \dirmany{En}{De, RANDOM} notation refers to a multilingual
model that was trained in the RANDOM experiment (randomly selected targets),
where one of the targets is German.
In the same way, \dirmany{En}{De, GERMANIC} refers to a model from
the GERMANIC experiment (targets selected from Germanic languages list).


%----------------------------------------------------------------------
\section{Training tools}

In the following section we describe the tools that are uset to implement
what was shown in Section \ref{section:method}.

\subsection{Toolkits}

There exists a number of different tools that can be used for training a NMT model.
General purpouse deep learning programming libraries like
Tensorflow\footnote{\url{https://tensorflow.org/}} and
PyTorch\footnote{\url{https://pytorch.org/}} are most popular for deep learning related
research. With their help it is possible to construct any of today's state-of-the-art
NMT models; pre-built and pre-trained models are initially present in such frameworks,
but it is also possible to describe a model from scratch.

Another option is presented by specialized NMT tool kits.
They usually contain efficient and tested implementations of NMT models as well as some of
usefull preprocessing tools.
For the experiments described in \ref{section:experiments} there is a need to train significant
amount of models with the same architecture and settings but different datasets.
Due to that fact, in this work the use of specialized NMT tool kit is more suitable.
Let us consider the foolowing list of broadly used tool kits as for year 2020,
presented in \cite{koehn_2020}:

\begin{itemize}
  \item OpenNMT (based on Torch/pyTorch)\footnote{\url{https://opennmt.net}}
  \item Sockeye (based on MXNet)\footnote{\url{https://github.com/awslabs/sockeye}}
  \item Fairseq (based on pyTorch)\footnote{\url{https://github.com/pytorch/fairseq}}
  \item Marian (stand-alone implementation in C++)\footnote{\url{marian-nmt.github.io}}
  \item Google's Transformer (based on Tensorflow)\footnote{\url{
    https://github.com/tensorflow/models/tree/master/official/transformer}}
  \item Tensor2Tensor (based on Tensorflow) \footnote{\url{
    https://github.com/tensorflow/tensor2tensor}}
\end{itemize}

We chose \textit{MARIAN-NMT} tool kit\footnote{\cite{mariannmt}} as a fast solution
with stable and efficient \textit{Transformer} \cite{vaswani-2017-transformer} implementation,
minimum of third-party dependencies, and ability to train models on multiple GPU units in parallel.


\subsection{Computational cluster}

In the experiments proposed above the expected number of models to be trained is quite big.
First of all, there should be 36 models for \textit{mono-target baseline} for En\to{}36 dataset.
For \textit{multi-target random} experiment the number is much bigger.
For example, let us consider a case with En\to{}3 models - each model translates from English to 
3 target languages. Specifying that each of 36 target languages from En\to{}36 dataset
should appear at least in 3 En\to{}3 models, series of random generation of En\to{}3 setups gave
the smallest amount of such setups equal to 44. For En\to{}5 case with 5 target languages in each
model and with the same restriction of minimum occurance the same procedure gave the
minimum amount of needed models equal to 34.

To be able to train large number of models in a reasonable amount of time we needed to use
computational cluster with GPU cards.
The computational clusters available at the institution are operating under
SGE\footnote{\url{https://arc.liv.ac.uk/trac/SGE}} scheduling software and are equipped with
GPU cards with minimum CUDA \textit{compute capability} 6.1.

Considering data storage quota limitation and high utilization of computational resources by
the cluster's users, the following training pipeline was designed:

\begin{outline}
    \1 Prepare task list
    \1 Iterate over the list working with at most N tasks in parallel
    \1 For each task
        \2 Subsample the dataset taking only those sentence pairs with target languages
	   specified in the task
	\2 Run the training procedure for limited amount of time (e.g. for one hour only)
	   starting with previous checkpoint if it already exists
	\2 Regularly compute metrics on the developement set and report them
	\2 On the event of evaluation on the developenemt set save the best model for each metric
	\2 After time is out the training is stopped and subsampled datasets are removed
    \1 If for next selected task the model is already trained then select next task from the list
    \1 If for next selected task the model is currently being trained then decrease
       the number N of tasks processed in parallel
\end{outline}


\subsection{Inspecting the training process}

As the number of models trained and being trained is growing, monitoring of the training
process becomes more and more complicated. If the experiments are also being run on different
computational clusters it becomes very possible that a parameter mistakenly set up to different
value or a corrupted dataset, or even hardware version may lead to an unexpected difference in results.

To address these and other issues that may occur during the training process we use
Weights$\&$Biases\footnote{\cite{wandb}} experiment tracking tool.
Its main features that are useful in this prospective are following:
\begin{outline}
	\1 Metric visualization
		\2 Training and validation loss curves
		(Figure \ref{fig:single-lang-group-vs-random-dashboard} left subplot)
		\2 Scatter plots (Figures \ref{fig:inspect-convergence}
		and \ref{fig:single-lang-group-vs-random-dashboard} middle subplot)
	\1 Artifact storage
		\2 Model checkpoints storage
			\3 stores 'heavy' model files which cannot be stored
			in \emph{git}
			\3 along with \emph{git} it makes possible to move training
			to the different computational cluster system
		\2 Sample translations of validation set
			\3 helps to observe improvements of translation quality
			in time
			\3 lets verify that model is actually produces meaningfull
			translation
	\1 Customizable reports
	\1 Hardware utilization
\end{outline}

\begin{sidewaysfigure}[b]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/slavic_cyrillic_bg.png}
	\mycaption{%
		Training progress dashboard for one translation direction%
	}{
		Here is a part of interactive report for `Slavic languages
		with Cyrillic script vs. random' experiment.
		In this specific case models' performance on Bulgarian part of
		validation set is compared. Note: the visualized BLEU scores are only
		used during the training and are not used for evaluation.

		\emph{Left}: \acrshort{bleu} score for \dir{En}{Bg} translation direction is monitored with
		training step on $X$ axis (top) and training epoch  (bottom).
		Each curve represents mean value (line) and its min/max value
		(range) at the point of time of multiple models' results.
		Models are grouped by the number of target languages and experiment subgroup
		(bilingual \dir{En}{Bg}, multilingual \dirmany{En}{Slavic} and \dirmany{En}{Random}).

		\emph{Middle}: Number of targets (axis $X$) vs.
		\acrshort{bleu} on \dir{En}{Bg} validation set (axis $Y$) vs.
		update steps (colos with scale at the top).

		\emph{Right}: Individual models' \dir{En}{Bg} validation \acrshort{bleu} scores.
	}
	\label{fig:single-lang-group-vs-random-dashboard}
\end{sidewaysfigure}


\begin{figure}[h]
	\begin{minipage}{0.8\textwidth}
	\centering
	\includegraphics[width=1.0\columnwidth]{img/inspect_overfit.png}
	\end{minipage}\hfill
	\begin{minipage}{0.8\textwidth}
	\centering
	\includegraphics[width=1.0\columnwidth]{img/inspect-bleu-vs-loss.png}
	\end{minipage}
	\mycaption{%
		Overall convergence dashboard%
	}{
		In these two interactive graphs, each point represents one model.
		Models that are currently training are visualized here together with
		completely converged models and those which training process is currently
		on hold.

		\emph{Top}: the $X$ axis represents the training loss value,
		the $Y$ axis represents the value for the same loss function calculated
		on the validation set. The color of each point represents current training
		epoch for the model. Normally for any model the point \fix{moves} from top right
		part of this graph to the bottom left part, representing both training and
		validation loss being gradually decreased during the training procedure.
		The point that moves to the middle left part of the graph may signalize about
		either \gls{overfitting} of the model on training set, or difference in data
		distribution in training and validation set, or else some mistake in training
		settings.
		This is useful for finding which training runs need attention and perhaps debugging.

		\emph{Right}: in this plot loss value on the validation set (axis $X$)
		is compared with geometric mean of \acrshort{bleu} scores
		for each of target languages.
		For any model during the training, its point usually \fix{move} from
		bottom right corner into the cluster of other points.
		The model the point of which `arrives' to any other location than
		the cluster may need special attention.
	}
	\label{fig:inspect-convergence}
\end{figure}


\cleardoublepage

\subsection{\todo{Model settings}}

The initial parameter selection is made with respect to \cite{training-tips}.
First of all, the hyperparameters of MT model are tuned
on couple of language pairs from one dataset.
The parameters leading to the same result in shorter time were preferred.
Then the selected parameters were used on all experimends with the dataset.


\subsubsection*{Tuning early stopping on early runs}

The initial \gls{early-stopping} setting was that after 5 consecutive
validation steps without improvement of validation \gls{loss} value
the training process stopped.
However, during the training of the first couple of bilingual models
the following situation has happened quite often:
further improving performance on validation set by
couple of tenths of BLEU points took as much time as reaching
the pre-optimal state.

In the \cref{fig:no_improvement_de}, it  can be seen that the path from
the beginning of training to the optimal point B (26.9 \acrshort{bleu})
took as much time as its further improvement by 0.2 BLEU
at point D (27.1 \acrshort{bleu}). However, there were certain models
with a bit bigger improvement after a much longer time, e.g.
0.8 \acrshort{bleu} points on Figure \ref{fig:no_improvement_fi}.

This behaviour makes our decision on where to stop particularly
complicated for multilingual models, as discussed in
\cref{section:finishing-the-training}.
After considering also some of preliminary multilingual runs,
the `patience' parameter of \gls{early-stopping} was set to 15.
After 15 consequtive validation steps without a metric improvement,
the training process is stopped.

\fix{it needs to be presented earlier and *commented including motivation*.
The red lines are unnatural at the first sight.}
In order to visually highlight when an increase in the validation score
is observed, we plot the number of ``steps stalled'',
see the red line in \cref{fig:no_improvement_de}.
The higher the diagonal line grows, the longer we have to wait for an improvement.
For instance, we see that the path..from beg to B took...

\begin{figure}[p]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/no_improvement_de.png}
	\mycaption{Example change of model's performance on validation set
		   in time}{
		Preliminary \dir{En}{De} model.

		\textit{Blue}: validation metric (value on the left axis in BLEU)

		\textit{Red}: validation metric (BLEU) stalled.
		Each consecutive validation step when the metric
		is not improved this value is incremented by 1.
		When the metric is improved this value is reset to 0.

		\textit{Green}: loss function value on validation set is
		stalled. Same logic as for \textit{Red}.

		BLEU score values at the points of improvement:
		$A$ -- 26.8, $B$ -- 26.9, $C$ -- 27.0, $D$ -- 27.1.
	}
	\label{fig:no_improvement_de}

	\vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

	\centering
	\includegraphics[width=1.0\columnwidth]{img/no_improvement_fi.png}
	\mycaption{Small improvement during long training}{
		In this case (\dir{En}{Fi}), the difference is a bit more
		visible: 21.3 at the first point and 21.9 at the best.
		Colors and scales are the same as at
		Figure \ref{fig:no_improvement_de}.
	}
	\label{fig:no_improvement_fi}
\end{figure}

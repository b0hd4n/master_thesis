\chapter{Experiment setup}
\label{chapter:experiment_setup}

In this chapter, we describe the data used for experiments, training setup
and experiments that were run to answer the questions asked in this thesis.

%----------------------------------------------------------------------
\section{{Questions and constraints}}
\label{section:questions_and_constraints}

Constraints:
\begin{displayquote}
	Translation quality for multi-lingual system is better or insignificantly
	worse than for mono-lingual one-to-one tranlsation system.

	Maximum possible target languages are combined in one model.
\end{displayquote}

Questions:
\begin{displayquote}
	How, \emph{on average}, does adding one more randomly selected target language
	to the multitarget model affect its En\to{}De performance?

	How is it different if we add a linguistically similar,
	not a randomly selected language?

	How does adding one more language from the same language family or group 
	\emph{on average} affect translation performance for a selected language
	pair (e.g. En\to{}De)?
\end{displayquote}



%----------------------------------------------------------------------
\section{Experiments}
\label{section:experiments}
%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Starting point}
\label{subsection:starting_point}

The approach described in \cref{section:multitarget_theory} with combining multiple
translation directions into the standart \emph{Transformer} model can also be used to
train just multi-target models, i.e. with one source language and multiple target languages.
In the following papers
(\perscite{arivazhagan-2019-mmnmt-in-the-wild}, \perscite{aharoni-etal-2019-massively}),
  the approach is furtherly developed, described and many different interesting cases are tested.
However, in each setting there is usually only one model of each kind considered.
For example, when \citet{aharoni-etal-2019-massively} compares 5-to-5,
25-to-25, 50-to-50, etc. models, there is only one 5-to-5 model, one 25-to-25, etc.

To conduct our experiments, we use this approach, but with the following differences:
\begin{itemize}
	\item We fix English as a source language, as we are exploring the multi-target experiments only, .
	\item We train multiple models for every translation direction and every setting.
	E.g. for the \dir{En}{De} translation direction and 1-to-5 setting there are
	couple of \dirmany{En}{De + 4 randomly selected targets} models.
	\item We use only up to 5 target languages in the model because of:
	\begin{itemize}
		\item limited resources;
		\item our selected datasets (which will be described in the next section)
		do not contain more than 5-6 languages of the same language group.
	\end{itemize}
\end{itemize}



%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Proposed experiments}
\label{subsection:proposed_experiments}

% Given the questions and constraints given in \ref{section:questions_and_constraints},
% the variable object in experiments is the data itself. Due to that, the setup similar
% to \cite{johnson-etal-2017-googles} was chosen. 

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsubsection*{Bilingual \gls{baseline}}

The purpose of training bilingual models is to have a reference point to be able to reason how
does every additional target language affects the model's performance.
\perscite{Siddhant2019} shows that using target language tags results
in the same model efficiency as separately encoding the target.
Therefore, we use target tags in this setting too, so that we can use
the same training pipeline.

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsubsection*{Multi-lingual baselines (RANDOM)}

For training multilingual models with a random set of target languages
the purpose is twofold: 
to show \acrshort{bleu} score decrease with increasing number of target languages and
to serve as a baseline for multitarget models with target languages grouped by
in a non-random way, e.g. by language group or linguistic similarity.


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsubsection*{Group by language group (SIMILAR)}

Multilingual models with a set of target languages from the same language group:
due to shared parts of vocabulary and linguistic properties we expect to
receive better results in comparison with multi-lingual baselines.
Ideally, the results would be comparable with bilingual baselines.


% \subsubsection*{Group by linguistic similarity}
% 
% From \perscite{siddhant-2020-x-ling-effect} follows that languages' script
% and similarly the amount of shared vocabulary is not so important
% for XX\to{}En translation direction.
% Example with Serbian and Croatian, with the same vocabulary but
% in different scripts.


%----------------------------------------------------------------------
\section{Dataset(s)}
\label{section:datasets}


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{{English to 36 languages}}
\label{subsection:en-to-36}

To observe effects of linguistic similarity of target languages,
it is important to examine enough possible variations of those.
The OPUS dataset (\cite{TIEDEMANN12.463}) is an open and free collection of texts
that covers more than 90 languages with data from several
domains.\footnote{Available at \url{http://opus.nlpl.eu/}} 

We made use of the sampling and splitting of the data created by the ELITR project.%
\footnote{\url{https://elitr.eu/wp-content/uploads/2019/07/D11.FINAL\_.pdf}}
For each of the language pairs and each sub-dataset
the data was split into training, validation and testing sets.
For each of the two latter sets, 2000 random sentences were selected
and the rest of the data remained for the training set.
In cases, where the sub-dataset contained less than 16000 sentence pairs,
no data was left for the validation set.
Later, for each language pair there were 1000000 sentence pairs
sampled from all training sub-sets.

The target domain in the ELITR project was combined of the public administration
domain and the spoken speech.
Due to this fact, the sentences were sampled from OPUS sub-datasets in this order:
firstly, if available, the sentences were taken from Europarl,
then EUbooks, OpenSubtitles, and then from all remaining sub-datasets.
The same procedure was used for validation set sentences per each language pair.
The test sets were left separate, so that the result on each domain would be observable.

The \acrshort{bpe} algorithm was used on both target and source sides of the
training set. After that, this vocabulary was used to preprocess (segment into subwords)
the training set, the validation set, and all the test sets.

For our experiments we used only the part of the resulting ELITR dataset where
the source language is English.
This way we received a dataset with one source (English) and 36 target 
(see full list in \cref{att:list_en-to-36}) languages.
Given that,
we decided to select these two groups of languages for the SIMILAR experiment:
\begin{itemize}
	\item Germanic group: Danish (da), German (de), Icelandic (is),
	Norwegian (no), Dutch (nl), and Swedish (sv).
	\item Slavic with cyrillic script: Bulgarian (bg), Macedonian (mk),
	Russian (ru), and Ukrainian (uk).
\end{itemize}

% -- table with bilingual results
\input{src/table_en-to-36_langs.tex}
% - - - - - -

We found an overlap in the source side of different language pairs.
Although this would not directly lead to unfair increase of the test score,
such sentence pairs were removed from the training sets.
This filtering decreased the number of sentence pairs
to 0.85-0.95 millions per language pair.

The data statistics for the resulting training set are displayed in \cref{fig:language_statistics}.
The first plot shows the amount of sentence pairs per translation direction, e.g.
the value for the `cs' column refers to the \dir{En}{Cs} part of the training set.
Because the source side is always in English, from the `Average \emph{source} sentence length'
plot (in subwords) we can deduce that for some languages the sentences are almost three times
as long as for the others, e.g. \dir{En}{Is} vs. \dir{En}{De}.
The shape of the curve displays the difference in domains of parts of the training set.
The `Average \emph{target} sentence length' curve is a bit different: the most possible
explanation is that different source languages are segmented differently and may have
different number of words for the same translation.
Unique subwords count at the source side reflects the diversity of the respective part
of the training set.
For the last plot, the higher number means that there were more words left as the whole
words and not split into subwords.

We also grouped the sub-datasets into groups by their domain
(see \cref{tab:subdatasets_groups}).
In the first group, the sub-datasets mostly consist of
either documents, or meeting proceedings.
The sentences are longer, drier and more formal.
In the second group, the datasets domains are news and
commentaries to the news articles.
The vocabulary is diverse and the sentences are less formal.
Spoken language transcripts are represented in the third group:
informal speech, spoken vocabulary, unfinished sentences.

As for groups 4 and 5, we expect to observe much lower results
in any experiment. The fourth group has much shorter sentences
and a very different vocabulary.
In the fifth group: the `Tanzil' consists of Quran texts, so its sentences'
structure and the vocabulary is very different from the most data in the training set;
The `Books' dataset contains sentences from 18th century books, so the issue is the same.
The `Wikipedia' consists of automaticaly aligned sentences.


\begin{table}[h!]
	\centering
	\begin{tabular}{c|p{0.5\columnwidth}|p{0.4\columnwidth}}
	\toprule
	     group & sub-dataset names  & description \\
	\midrule
	 1 &  Europarl/vx, DGT, MultiUN, EUbookshop, JRC-Acquis,
	      ECB, EMEA
	   &  Proceedings and documents from Europarl, UN, etc. \\
	 2 &  NewsCommentary, GlobalVoices, WMT-News 
	   &  News articles and commentaries \\
	 3 &  OpenSubtitles, Tatoeba
	   &  Short sentences, human speech, general domain \\
	 4 &  OpenOffice, PHP, KDE4, Gnome
	   &  Software documentation or interface elements \\
	 5 &  Tanzil, Books, Wikipedia
	   &  Other  \\
	\bottomrule
	\end{tabular}

	\mycaption{Groups of subdatasets in the \emph{en-to-36} dataset}{}
	\label{tab:subdatasets_groups}
\end{table}

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/train_set_statistics.png}
	\mycaption{Training data language statistics}{
		Languages that could be observed on the $X$ axis, are sorted in the same way as in \cref{att:list_en-to-36}.
		From top to bottom:
		total number of sentence pairs in training set per language,
		average number of subwords per sentence on the source side,
		the same on the target side,
		total number of unique subwords for this target language on the source side,
		the same on the target side.
	}
	\label{fig:language_statistics}
\end{figure}



%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{{UN parallel corpus: English to 5 languages}}
\label{subsection:en-to-5}

For an additional experiment about adding non-related target languages
into the mix, we used `The UN Parallel Corpus v1.0'
\parcite{ziemski-etal-2016-united}.
It consists of more that 11 millions sentence 6-tuples in six UN languages,
i.e. every English sentence has a translation into Arabic (ar), Chinese (zh),
French (fr), Russian (ru), and Spanish (es).

First, we split the data into training, validation and test sets.
We selected 30000 sentence tuples into the test set, then from the remaining
data we took 30000 sentence tuples into the validation set.
After that all sentence tuples that overlapped the test set in the same way as in the \cref{subsection:en-to-36},  were removed, which gave us 10987284 sentence tuples

After that, we divided each 6-tuple into five sentence pairs,
where English was selected as a source language and all other languages were target languages. We also added the target language tag as seen in  \cref{subsection:en-to-36}.

As a result, we received a dataset in the same format as the \gls{en-to-36}.
The main differences of the new dataset are:
\begin{itemize}
	\item the size: training data for each translation direction
		contains ten times more sentence pairs;
	\item variety: the training, validation, and test sets
		consist of data from the same domain;
	\item only one test set, due to the previous point.
\end{itemize}
When using the same model as for \gls{en-to-36}, these differences may
allow us seeing how the model's translation quality changes if we add
more target languages in the mix.
In the following text the `en-to-5' notation refers to this dataset.
We specify explicitly, if this dataset is used in a experiment;
otherwise the en-to-36 dataset is used.


%----------------------------------------------------------------------
\section{Method}
\label{section:method}

In this section we describe how the models are trained, which metrics
are collected and how are they analyzed.


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Training tasks}
\label{section:training_tasks}

We define a task as a set of models that needs to be trained.
In the training pipeline, the task is represented by a plain text file,
in which each row contains a set of target languages for one model.
In \cref{exmp:task}, a task for training three bilingual models looks like in

\vspace{\baselineskip}
\begin{minipage}[t]{0.9\textwidth}
    germanic\_2.task:
    
    ---------------
    
    da de
    
    da is
    
    de is

	\begin{exmp}
	The task is to train three models with target languages sampled from
	Germanic group: \dirmany{En}{Da, De} (from English to Danish and German),
	\dirmany{En}{Da, Is} and \dirmany{En}{De, Is}.

	\label{exmp:task}
	\end{exmp}
\end{minipage}
\vspace{\baselineskip}


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{{Data preprocessing and selection}}
\label{section:data_selection}
\label{section:data_preprocessing}

\subsubsection*{Preprocessing}
	Before any sampling both en-to-36 and en-to-5 datasets
were preprocessed  in the following way:
\begin{itemize}
	\item the vocabulary was created using the \acrshort{bpe}
		on both target and source parts of the training set only;
	\item using the vocabulary, the train set, the validation
		set, and the test set were segmented and then stored
		in this preprocessed form.
\end{itemize}

It must also be noted, that due to the fact that every sentence starts from a
target tag, these target tags are not splitted by the \acrshort{bpe}.

\subsubsection*{Dataset subsampling}

Both training sets from \cref{section:datasets} contain data for
many language pairs, but in order to train some specific model,
we do not need them all.
The same holds for the validation sets.

Therefore, for each specific model \dirmany{En}{L1, L2, ...}
we need to subsample the \emph{\dirmany{En}{L1, L2, ...} training set}
from \emph{the whole training set} (e.g. \gls{en-to-36}),
the \emph{\dirmany{En}{L1, L2, ...} validation set}.



\subsubsection*{Training set}

To prepare a training set for a specific model, we need to select
sentence pairs with relevant source languages from
\emph{the whole training set}.
This step is done in the same way for both \gls{en-to-5} and \gls{en-to-36}
datasets.
Further, the notation `\emph{\dirmany{En}{L1, L2, ...} training set}'
refers to a subsampled in this way training set.

For example, let us take the \dirmany{En}{Fr, De} setup, which means that
the model to be trained should take a source sentence in English and
produce translation either in French or in German.
The language of the model's output depends on the target tag at the beginning
of the input sentence, i.e. \tagto{fr} tag in source sentence leads to French
target language.

To train such a model, only related sentence pairs are subsampled
from the whole training set.
In this case, from \emph{the whole training set} we select only those sentence
pairs, which source side starts with tags \tagto{fr} or \tagto{de}.
Such a subsampled dataset is then used to train this model, and referred to 
as the \\ \emph{\dirmany{En}{Fr, De} training set}.


\subsubsection*{Validation set}

For any model the validation set is constructed from the big validation set
by selecting only relevant sentence pairs in the same way as the training set,
i.e. pairs with the target in one of the examined languages.
For the example setup from above, \dirmany{En}{De,Fr}, the validation set
consists of an equal amount of \dir{En}{De} and \dir{En}{Fr} sentence pairs.
E.g., if in the complete validation set there are 1000 sentence pairs for
each of possible target languages, then for \dirmany{En}{De,Fr}
model the validation set will contain 2000 sentence pairs, and for
\dirmany{En}{De,Es,Fr} it will contain 3000 sentence pairs.

\subsubsection*{Test set}

For experiments with \gls{en-to-5} dataset (Section \ref{subsection:en-to-5})
the test sets are created in the same way as the validation sets.
For \gls{en-to-36} dataset (Section \ref{subsection:en-to-36})
the test set is divided on subsets by the source dataset.
It means, that for each of the source datasets (like OpenSubtitles/v11, 
Europarl/v7, etc.) there exists a separate test set.

\subsubsection*{Example}
In the experiments proposed above the expected number of models to be trained is quite big.
First of all, there should be 36 models for \textit{mono-target baseline} for En\to{}36 dataset.
For the \textit{multi-target random} experiment the number is much bigger.
For example, let us consider a case with En\to{}3 models, where each model translates from English to 
3 target languages. Specifying that each of 36 target languages from En\to{}36 dataset
should appear at least in 3 En\to{}3 models, series of random generation of En\to{}3 setups gave
the smallest amount of such setups equal to 44. For En\to{}5 case with 5 target languages in each
model and with the same restriction of minimum occurance the same procedure gave the
minimum amount of needed models equal to 34.



%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Model settings}

The initial parameter selection is made with respect to \cite{training-tips}.
First of all, the hyperparameters of MT model are tuned
on couple of language pairs from one dataset.
The parameters leading to the same result in shorter time were preferred.
Then the selected parameters were used on all experimends with the dataset.

\subsubsection*{Model settings}

In all the experiments, models were trained with the same parameters.
It is a vanilla Transformer model from \cref{section:transformer} with the following
settings:

\begin{verbatim}
# General options
workspace: 7000
seed: 1111
# Model options
type: transformer
layer-normalization: true
tied-embeddings-all: true
# Training options
sync-sgd: true
cost-type: ce-mean-words
dim-vocabs:
  - 42671
  - 42671
disp-freq: 100
save-freq: 100
overwrite: true
max-length: 500
devices:
  - 0
  - 1
cpu-threads: 0
mini-batch-fit: true
maxi-batch-sort: src
exponential-smoothing: 0.0001
learn-rate: 0.0004
lr-warmup: 4000
lr-decay-inv-sqrt: 4000
lr-report: true
clip-norm: 1
transformer-dropout: 0.1
label-smoothing: 0.1
optimizer-delay: 8
optimizer-params:
  - 0.9
  - 0.98
  - 1e-09
# Validation set options
valid-freq: 1000
valid-metrics:
  - translation
  - ce-mean-words
early-stopping: 15
beam-size: 6
normalize: 0.6
keep-best: true
\end{verbatim}

\subsubsection*{Tuning early stopping on early runs}

The initial \gls{early-stopping} setting was such, that after 5 consecutive
validation steps without improvement of validation \gls{loss} value
the training process would stop.
However, during the training of the first couple of bilingual models
the following situation has happened quite often:
further performance improvement on validation set by
couple of tenths of BLEU points took as much time as reaching
the pre-optimal state.

In the \cref{fig:no_improvement_de}, it  can be seen that the path from
the beginning of training to the optimal point B (26.9 \acrshort{bleu})
took as much time as its further improvement by 0.2 BLEU
at point D (27.1 \acrshort{bleu}). However, there were certain models
with a bit bigger improvement after a much longer time, e.g.
0.8 \acrshort{bleu} points on Figure \ref{fig:no_improvement_fi}.
In order to visually highlight when an increase in the validation score
is observed, we plot the number of ``steps stalled'',
see the red line in \cref{fig:no_improvement_de}.
The higher the diagonal line grows, the longer we have to wait for an improvement.
For instance, we see the path from A to B, where the validation
metric was not improving for 13 validation steps in \cref{fig:no_improvement_de}.

This behaviour makes our decision on where to stop the training process particularly
complicated for multilingual models, as discussed in
\cref{section:finishing-the-training}.
After considering also some of preliminary multilingual runs,
the `patience' parameter of \gls{early-stopping} was set to 15.
After 15 consequtive validation steps without a metric improvement,
the training process is stopped.


\begin{figure}[p]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/no_improvement_de.png}
	\mycaption{Example change of model's performance on validation set
		   in time}{
		Preliminary \dir{En}{De} model.

		\textit{Blue}: validation metric (value on the left axis in BLEU)

		\textit{Red}: validation metric (BLEU) stalled.
		Each consecutive validation step when the metric
		is not improved this value is incremented by 1.
		When the metric is improved this value is reset to 0.

		\textit{Green}: loss function value on validation set is
		stalled. Same logic as for \textit{Red}.

		BLEU score values at the points of improvement:
		$A$ -- 26.8, $B$ -- 26.9, $C$ -- 27.0, $D$ -- 27.1.
	}
	\label{fig:no_improvement_de}

	\vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

	\centering
	\includegraphics[width=1.0\columnwidth]{img/no_improvement_fi.png}
	\mycaption{Small improvement during long training}{
		In this case (\dir{En}{Fi}), the difference is a bit more
		visible: 21.3 at the first point and 21.9 at the best.
		Colors and scales are the same as at
		Figure \ref{fig:no_improvement_de}.
	}
	\label{fig:no_improvement_fi}
\end{figure}

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Training}


During the training procedure, the checkpointing of the model occurs once per specified number of updates.
The model weights are saved to the disk and a set of measurements is logged.

\begin{samepage}
\begin{itemize}
	\item [Those measurements are:]
	\item training loss value (mean value for all updates since
	last checkpoint)
	\item learning rate value
	\item training speed (processed words per second)
	\item training time since last checkpoint
	\item number of updates happened from the beginning till this checkpoint
\end{itemize}
\end{samepage}
\begin{samepage}
Hardware usage should also be recorded if possible:
\begin{itemize}
	\item GPU usage
	\item CPU usage
	\item memory usage
	\item disk I/O
	\item network I/O
\end{itemize}
\end{samepage}

The hardware metrics are not important for model's evaluation
but may help early spot mistakes like underuse of GPU or CPU, lack of RAM, etc.
This is why they could possibly be recorded continuously. 

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Validation}
\label{subsection:validation}

The validation set is used to track model's performance during the training
on an unseen set of data and to perform early stopping.
This model-specific validation set is created as described
in \cref{section:data_selection}.
These measurements are only used during the training and not for the evaluation.

Once per specified number of steps the validation occurs:
validation metrics are recorded, for any metric which value was
improved current model weights are saved as best model by this metric.
If early stopping condition occurred, then the training process is stopped.

For the validation set, we collect not only the loss function value
but also the metric of interest, which is \acrshort{bleu} score.
However, this \acrshort{bleu} scores are not used for the model's
evaluation but only during the training process.
The \acrshort{bleu} of the whole model's validation set
is not something we are interested in.
For the discussed example we collect validation bleu:fr and bleu:de scores
which represent \acrshort{bleu} scores for French and German
parts of validation set.
E.g., to compute bleu:fr we select only En\to{}Fr sentence pairs from the
validation set.

Also, an aggregated value of the bleu:xx scores, i.e. the mean of BLEU scores
over all target languages of the current model, is also recorded
and may be used for early stopping: ending the training process
when the metric is not improved during last N validation steps.

\begin{samepage}
Altogether, the following validation metrics are recorded after the
validation step:
\begin{itemize}
	\item loss function value
	\item bleu:xx which is \acrshort{bleu} score for each of
	model's target languages
	\item aggregated value of all bleu:xx values
	\item translation time of the model's validation set
\end{itemize}
\end{samepage}

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Finishing the training}
\label{section:finishing-the-training}

When should we stop the training?
It is not possible to say precisely when the model will acquire its best performance because of stochastic nature of
the training algorithm (\acrshort{sgd}).
Because of that we need to use some method to decide when
training process should be stopped.


\subsubsection*{Number of \glspl{epoch}}

The easiest approach is to specify the number of \glspl{epoch}
after which the training is stopped.
This could be a good solution for the case when all models
that will be compared are trained on the same amount of data from
the same domain.
But in our case, adding one more target language adds a constant
amount of sentence pairs to the training set.
Roughly, if the number of \glspl{epoch} is specified as a stop
condition, a bilingual \dir{En}{De} model will see the German
training data $x$ times, when multilingual \dir{En}{De, Fr, Es}
will only see the German training data $x / 3$ times.

%What value should be used to compare model's performace in time?
%The first and the most obvious approach 
\subsubsection*{Early stopping}

\Gls{early-stopping} is a regularization technique used to avoid
possible \gls{overfitting} of a model on the training data.
In general, it works in the following way: after every validation step
it checks if the metric value improved during last $N$ validations.
The metric to be controlled and number of validation steps $N$ are
the parameters of this method
(see \cref{fig:early_stopping}).

Another situation is even more probable in the area of NMT with generally
large training datasets: model's validation performance
is either stalled or slightly improved
(see \cref{fig:early_stopping_no_improvement}).
In this case \gls{early-stopping} helps to avoid unnecessary spendings
on computational resources.

\begin{samepage}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/early_stopping.png}
	\mycaption{\Gls{early-stopping} to prevent overfitting
		(Fig. 1 from \citet{early-stopping-img})}{
		At the `early stopping' point the model's performance
		on unseen validation set of data does not improve
		anymore. Further training leads to poorer performance
		on unseen data. Stopping the training at this point
		results in better model's performance on unseen data.
	}
	\label{fig:early_stopping}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\columnwidth]{img/early_stopping_no_improvement.png}
	\mycaption{\Gls{early-stopping} as the model is not improving}{
		Even though the metric value on the training set is
		still slowly improving, its value on the unseen
		validation set is stalled. Further spending of 
		computational resources is unjustified.
	}
	\label{fig:early_stopping_no_improvement}
\end{figure}
\end{samepage}

In our case, we could use \gls{early-stopping} to ensure more equal conditions
for models with different sizes of training data.
A suitable number $N$ could be found experimentaly, but which metric should be used?

Given that the task is to train a model that is as good as possible in
\textbf{all} of its target directions, the \acrshort{bleu} score of
the whole translated validation set for this set of languages does not
say anything about the model's performance in each specified
translation direction.

\subsubsection*{Aggregated value of BLEU scores}

Therefore, we should use separate BLEU scores which represent
model's performance in each of translation directions.
The most intuitive and naive way is to compare BLEU scores
for each target language.

However, most of frameworks and toolkits can monitor only one metric
for the early stopping.
Considering that different validation \acrshort{bleu} scores
are computed for different parts of the validation set 
which are also in different languages, they cannot be directly compared
and may have different scale.

For example, a model for the \dirmany{En}{De,Fr} direction is being
trained.
Before the moment, an \dir{En}{De} model has already been trained and
had the best \acrshort{bleu} score of 25 on the German part of the
validation set.
A \dir{En}{Fr} model has also been trained, and its result on the
French part of the validation score is 35.
So for the currently training \dirmany{En}{De,Fr}, one percentage point
change for the \dir{En}{De} direction is not equal to the same change for the
\dir{En}{Fr} direction.

Geometric mean is known to be good for aggregating multiple metrics
with different scale (see \cref{eq:geometric_mean}).

\begin{equation}
\label{eq:geometric_mean}
	geometric\_mean = \left(\prod _{i=1}^{n}x_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}
\end{equation}

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Testing}
\label{subsection:testing}

After the training is finished, the received models should be evaluated on unseen
test data.
After translating the test set for each of target directions of the model
the following record is created:
\begin{itemize}
	\item model name
	\item source language
	\item target languages
	\item tested target language
	\item \acrshort{bleu} score for this part of translation
	\item metric, based on which the best model was saved
	\item dataset name (for \gls{en-to-36})
\end{itemize}

Let us return to the example setup is \dirmany{En}{De, Fr} and
suppose the reported validation metrics are the mean loss function value on
test set and 'translation' (geometric mean of all reported BLEU scores,
see Section \ref{subsection:validation}).
After the training is finished, there will be two models: the best by
the loss value and the best by 'translation'.
Records are then created for each of them: for \dir{En}{Fr} translation
and for \dir{En}{Es}. In total, 4 results are recorded.

If the model was trained and tested on \gls{en-to-36} dataset,
then $n$ records are created 4 times, where $n$
is the number of the OPUS subdataset from which the data was sampled.


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Analysis}

After the required set of models is trained and their test
metrics are collected, data should be analysed.

For example, let us take these four models: \dirmany{En}{De, Fr},
\dirmany{En}{De, Az}, \dirmany{En}{De, Bg}, and \dirmany{En}{Bg, Az}.
After the training, they provide us with three results for \dir{En}{De}
direction 2-target baseline,
one value for \dir{En}{Fr},
two values for \dir{En}{Bg}
and two for \dir{En}{Az}.
These aggregated \dirmany{En}{De, X} results will be later compared with
aggregated \dirmany{En}{De, X1, X2} for three target languages,
\dirmany{En}{De, X1, X2, X3} for four target languages, where X1, ... X$i$ are
some other targets.

Next, the \dirmany{En}{De, RANDOM} notation refers to a multilingual
model that was trained in the RANDOM experiment (randomly selected targets),
where one of the targets is German.
In the same way, \dirmany{En}{De, GERMANIC} refers to a model from
the GERMANIC experiment (targets selected from Germanic languages list).


%----------------------------------------------------------------------
\section{Training tools}

In the following section we describe the tools that are used for implementing 
what was shown in Section \ref{section:method}.

%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Toolkits}

There is a number of different tools that can be used for training a NMT model.
General purpouse deep learning programming libraries like
Tensorflow\footnote{\url{https://tensorflow.org/}} and
PyTorch\footnote{\url{https://pytorch.org/}} are the most popular for deep learning related
research. With their help it is possible to construct any of today's state-of-the-art
NMT models; pre-built and pre-trained models are initially present in such frameworks,
but it is also possible to describe a model from scratch.

Another option is presented by specialized NMT toolkits.
They usually contain efficient and tested implementations of NMT models as well as some of
the usefull preprocessing tools.
For the experiments described in \ref{section:experiments} there is a need to train a significant
amount of models with the same architecture and settings but different datasets.
Due to that fact, in this work the use of specialized NMT toolkit is more suitable.
Let us consider the foolowing list of broadly used tool kits as for year 2020,
presented in \cite{koehn_2020}:

\begin{itemize}
  \item OpenNMT (based on Torch/pyTorch)\footnote{\url{https://opennmt.net}}
  \item Sockeye (based on MXNet)\footnote{\url{https://github.com/awslabs/sockeye}}
  \item Fairseq (based on pyTorch)\footnote{\url{https://github.com/pytorch/fairseq}}
  \item Marian (stand-alone implementation in C++)\footnote{\url{marian-nmt.github.io}}
  \item Google's Transformer (based on Tensorflow)\footnote{\url{
    https://github.com/tensorflow/models/tree/master/official/transformer}}
  \item Tensor2Tensor (based on Tensorflow) \footnote{\url{
    https://github.com/tensorflow/tensor2tensor}}
\end{itemize}

We chose \textit{MARIAN-NMT} tool kit\footnote{\cite{mariannmt}} as a fast solution
with stable and efficient \textit{Transformer} \cite{vaswani-2017-transformer} implementation,
minimum of third-party dependencies, and ability to train models on multiple GPU units in parallel.


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Computational cluster}

To be able to train large number of models in a reasonable amount of time we needed to use
computational cluster with GPU cards.
The computational clusters available at the institution are operating under
SGE\footnote{\url{https://arc.liv.ac.uk/trac/SGE}} scheduling software and are equipped with
GPU cards with minimum CUDA \textit{compute capability} 6.1.


\subsection{Training pipeline}
\label{section:training_pipeline}

Considering data storage quota limitation and high utilization of computational resources by
the cluster's users, the following training pipeline was designed:

\begin{outline}
    \1 Prepare task list
    \1 Iterate over the list working with at most N tasks in parallel
    \1 For each task:
        \2 Subsample the dataset taking only those sentence pairs with target languages
	   specified in the task;
	\2 Run the training procedure for limited amount of time (e.g. for one hour only)
	   starting with previous checkpoint if it already exists;
	\2 Regularly compute metrics on the developement set and report them;
	\2 On the event of evaluation on the developenemt set save the best model for each metric;
	\2 After time is out the training is stopped and subsampled datasets are removed.
    \1 If the model is already trained for the next selected task,  select the next task from the list;
    \1 If the model is currently being trained for the next selected task, decrease
       the number N of tasks processed in parallel.
\end{outline}


%-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
\subsection{Inspecting the training process}

As the number of trained models, as well as models that are being trained is growing, monitoring of the training
process becomes more and more complicated. If the experiments are also being run on different
computational clusters it becomes very possible that a parameter mistakenly set up to different
value or a corrupted dataset, or even hardware version may lead to an unexpected difference in results.

To address these and other issues that may occur during the training process we use
Weights$\&$Biases\footnote{\cite{wandb}} experiment tracking tool.
Its main features that are useful in this prospective are the following:
\begin{outline}
	\1 Metric visualization
		\2 Training and validation loss curves
		(Figure \ref{fig:single-lang-group-vs-random-dashboard} left subplot)
		\2 Scatter plots (Figures \ref{fig:inspect-convergence}
		and \ref{fig:single-lang-group-vs-random-dashboard} middle subplot)
	\1 Artifact storage
		\2 Model checkpoints storage
			\3 stores 'heavy' model files which cannot be stored
			in \emph{git}
			\3 along with \emph{git} it makes possible to move training
			to the different computational cluster system
		\2 Sample translations of validation set
			\3 helps to observe improvements of translation quality
			in time
			\3 lets verify that model is actually produces meaningfull
			translation
	\1 Customizable reports
	\1 Hardware utilization
\end{outline}

\begin{sidewaysfigure}[b]
	\centering
	\includegraphics[width=1.0\columnwidth]{img/slavic_cyrillic_bg.png}
	\mycaption{%
		Training progress dashboard for one translation direction%
	}{
		Here is a part of interactive report for `Slavic languages
		with Cyrillic script vs. random' experiment.
		In this specific case models' performance on Bulgarian part of
		validation set is compared. Note: the visualized BLEU scores are only
		used during the training and are not used for evaluation.

		\emph{Left}: \acrshort{bleu} score for \dir{En}{Bg} translation direction is monitored with
		training step on $X$ axis (top) and training epoch  (bottom).
		Each curve represents mean value (line) and its min/max value
		(range) at the point of time of multiple models' results.
		Models are grouped by the number of target languages and experiment subgroup
		(bilingual \dir{En}{Bg}, multilingual \dirmany{En}{Slavic} and \dirmany{En}{Random}).

		\emph{Middle}: Number of targets (axis $X$) vs.
		\acrshort{bleu} on \dir{En}{Bg} validation set (axis $Y$) vs.
		update steps (colos with scale at the top).

		\emph{Right}: Individual models' \dir{En}{Bg} validation \acrshort{bleu} scores.
	}
	\label{fig:single-lang-group-vs-random-dashboard}
\end{sidewaysfigure}


\begin{figure}[h]
	\begin{minipage}{0.8\textwidth}
	\centering
	\includegraphics[width=1.0\columnwidth]{img/inspect_overfit.png}
	\end{minipage}\hfill
	\begin{minipage}{0.8\textwidth}
	\centering
	\includegraphics[width=1.0\columnwidth]{img/inspect-bleu-vs-loss.png}
	\end{minipage}
	\mycaption{%
		Overall convergence dashboard%
	}{
		In these two interactive graphs, each point represents one model.
		Models that are currently training are visualized here together with
		completely converged models and those which training process is currently
		on hold.

		\emph{Top}: the $X$ axis represents the training loss value,
		the $Y$ axis represents the value for the same loss function calculated
		on the validation set. The color of each point represents current training
		epoch for the model. Normally for any model the point moves from top right
		part of this graph to the bottom left part, representing both training and
		validation loss being gradually decreased during the training procedure.
		The point that moves to the middle left part of the graph may signalize about
		either \gls{overfitting} of the model on training set, or difference in data
		distribution in training and validation set, or else some mistake in training
		settings.
		This is useful for finding which training runs need attention and perhaps debugging.

		\emph{Bottom}: in this plot loss value on the validation set (axis $X$)
		is compared with geometric mean of \acrshort{bleu} scores
		for each of target languages.
		For any model during the training, its point usually moves from
		bottom right corner into the cluster of other points with each validation step.
		The model, the point of which `arrives' to any other location than
		the cluster may need special attention.
	}
	\label{fig:inspect-convergence}
\end{figure}


\cleardoublepage
